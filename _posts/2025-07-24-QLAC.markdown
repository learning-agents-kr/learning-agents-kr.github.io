---
layout: post
title: "[ë…¼ë¬¸ ë¦¬ë·°] Sample-Efficient Reinforcement Learning with Action Chunking"
date: 2025-7-24 20:30:00 +0300
img: 250724/image.png # Add image post (optional)
presenter: ì´ë™ì§„
tags: [ì´ë™ì§„]
use_math:  true
---

<div class="note-box" markdown="1">
<p class="note-box-title">ë…¼ë¬¸ ì •ë³´</p>

**ì œëª©:** Sample-Efficient Reinforcement Learning with Action Chunking

**ì €ì:** Qiyang Li, Zhiyuan Zhou, Sergey Levine, UC Berkeley.

**í•™íšŒ:** NeurIPS 2025

**ë§í¬:** [https://arxiv.org/abs/2507.07969](https://arxiv.org/abs/2507.07969)

</div>

<br>

# Overview

- **Target task:** Offline2Online, sample efficient RL, Exploration
- **Algorithm class:** FQL (Flow Q-learning; [[1]](#ref1)) with an extended action space
- **Motivation**
    1. slow 1-step TD backups
    2. temporally incoherent actions for exploration
    - **[Comment]** Offline2onlineì—ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œì ì„ í•´ê²°í•˜ëŠ” ë…¼ë¬¸ì€ ì•„ë‹ˆë‹¤. 1ë²ˆì€ sample efficient RLê³¼ ì—°ê´€ë˜ê³ , 2ë²ˆì€ explorationê³¼ ì—°ê´€ë˜ê¸° ë•Œë¬¸ì— ì¼ë°˜ì ì¸ online RL ë¶„ì•¼ì—ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œë¥¼ ë‹¤ë£¨ëŠ” ë…¼ë¬¸
- **Solution: Action chunking**
    - í˜„ì¬ ì‹œì ì˜ í–‰ë™ë§Œ ì¶œë ¥í•˜ëŠ” ê¸°ì¡´ ì •ì±… ëŒ€ì‹  **í˜„ì¬ë¶€í„° $h$ steps ë™ì•ˆ ì‚¬ìš©í•  action sequenceë¥¼ ì¶œë ¥í•˜ëŠ” ì •ì±… ì‚¬ìš©**:
        
        $$
        \pi_\psi(\mathbf{a}_{t:t+h-1} \mid s_t) \quad\text{where}\quad \mathbf{a}_{t:t+h-1}= [\overbrace{a_t \;\; a_{t+1} \;\;  \ldots \;\;  a_{t+h-1}}^{h \;\text{times}}]^\top
        $$
        
    - **Open-loop control:** $s_t$ë§Œ ë³´ê³  $a_t$ë¶€í„° $a_{t+h-1}$ì„ ìˆœì„œëŒ€ë¡œ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©
    
    ![$h=5$ ì˜ˆì‹œ.]({{site.baseurl}}/assets/img/250724/image.png)
    
    <center>$h=5$ ì˜ˆì‹œ.</center>
    
<br>

---

# Introduction

**ê°€ì¥ í° motivation**

- ìµœê·¼ imitation learning (IL) ë¶„ì•¼ì—ì„œ action chunkingì„ ì‚¬ìš©í•œ ì—°êµ¬ ë“±ì¥ (ALOHA ë…¼ë¬¸; [[2]](#ref2))

<br>

â“ Action chunkingì´ IL ë¶„ì•¼ì—ì„œëŠ” ì‚¬ìš©ë˜ì§€ë§Œ RLì—ì„œëŠ” ì‚¬ìš©ë˜ì§€ ì•Šì€ ì´ìœ 

- IL ë°ì´í„°ì…‹ (íŠ¹íˆ ì¸ê°„ì´ ì§ì ‘ ìˆ˜ì§‘í•œ ë°ì´í„°ì…‹)ì€ non-Markovian behaviorì„ í¬í•¨í•˜ê³  ìˆì„ ìˆ˜ ìˆìŒ
- í•˜ì§€ë§Œ MDPë¡œ ë¬¸ì œë¥¼ formulateí•˜ëŠ” RLì˜ ê²½ìš° optimal policyê°€ Markovian poilcyì´ê¸° ë•Œë¬¸ì— action chunking policyëŠ” suboptimalí•˜ë‹¤.

<br>

ğŸ’¡ í•˜ì§€ë§Œ online RLì—ì„œ action chunkingì´ íš¨ê³¼ì ì¼ ìˆ˜ë„ ìˆëŠ” ì´ìœ 

- Optimal policyì„ ì°¾ê¸° ìœ„í•´ ê¼­ í•„ìš”í•œ ê³¼ì •ì¸ explorationì˜ ê²½ìš° non-Markovian ì„±ê²©ì„ ë ë©° temporally extended actionsê°€ ë” íš¨ê³¼ì ì¼ ìˆ˜ ìˆë‹¤ê³  ì£¼ì¥
- 1-step TDì˜ ê²½ìš° value backupê°€ ëŠë ¤ sample inefficientí•˜ë‹¤.

<br>

---

# Q-Learning with Action Chunking (Q-LAC)

- í˜„ì¬ ì‹œì ì˜ í–‰ë™ë§Œ ì¶œë ¥í•˜ëŠ” ê¸°ì¡´ ì •ì±… ëŒ€ì‹  **í˜„ì¬ë¶€í„° $h$ steps ë™ì•ˆ ì‚¬ìš©í•  action sequenceë¥¼ ì¶œë ¥í•˜ëŠ” ì •ì±… ì‚¬ìš©**:
    
    $$
    \pi_\psi(\mathbf{a}_{t:t+h-1} \mid s_t) \quad\text{where}\quad \mathbf{a}_{t:t+h-1}= [\overbrace{a_t \;\; a_{t+1} \;\;  \ldots \;\;  a_{t+h-1}}^{h \;\text{times}}]^\top
    $$
    
<br>

- **Open-loop control:** $s_t$ë§Œ ë³´ê³  $a_t$ë¶€í„° $a_{t+h-1}$ì„ ìˆœì„œëŒ€ë¡œ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©

![image.png]({{site.baseurl}}/assets/img/250724/image.png)

<br>

<details style="padding-left: 25px;">
<summary><p style="display:inline-block; padding-left:2px;">ì´ decision processë¥¼ ê¸°ì¡´ Markov decision process (MDP)ë¡œë¶€í„° ë‹¤ì‹œ formulation ê°€ëŠ¥</p></summary>
<div markdown="1" style="padding-left:15px;">

- ê¸°ì¡´ MDP $\mathcal{M}=(\mathcal{S}, \mathcal{A}, r, p, \gamma)$ â‡’ ìƒˆë¡œìš´ MDP $\bar{\mathcal{M}}=(\mathcal{S}, \bar{\mathcal{A}}, \bar{r}, \bar{p}, \gamma^h)$
- ì›ë˜ í–‰ë™ê³µê°„ $\mathcal{A}\subseteq\mathbb{R}^{d_a}$ ëŒ€ì‹  extended action space

    $$
    \bar{\mathcal{A}}\subseteq\mathbb{R}^{\overbrace{d_a \times d_a \times \ldots \times d_a}^{h \, \text{times}}}
    $$
    
- ë³´ìƒí•¨ìˆ˜ ($h$ steps ë™ì•ˆ ë°›ì€ ë³´ìƒì˜ discounted sum)
    
    $$
    \bar{r}(s_t, \mathbf{a}_{t:t+h-1})=\sum_{k=0}^{h-1}\gamma^{k} r(s_{t+k},a_{t+k})
    $$
    
- ì „ì´í™•ë¥ ë¶„í¬ ($s_t$ì˜ ë‹¤ìŒ ìƒíƒœê°€ $s_{t+h}$ì´ ë¨)

$$
\bar{p}(s_{t+h} \mid s_t, \mathbf{a}_{t:t+h-1})=\prod_{k=0}^{h-1}p(s_{t+k+1} \mid s_{t+k},a_{t+k})
$$

<br>

</div>
</details>
    
<br>

- ë§¤ $h$ stepsë§ˆë‹¤ transition $(s, a, r, s')$ ì„ ì €ì¥í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‰½ê²Œ êµ¬í˜„ë¨

    $$
    \mathcal{D}=\lbrace \left(s_t,\mathbf{a}_{t:t+h-1},\sum_{k=0}^{h-1}\gamma^{k} r_{t+k}, s_{t+h}\right) : t=1, h,2h,\ldots, T.\rbrace
    $$

<br>

- ìœ„ transitionì„ ì‚¬ìš©í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ critic lossì™€ actor loss ë¥¼ ì–»ìŒ

    $$
    \mathcal{L}(\theta)=\mathbb{E}_{\mathcal{D}}\left[ \left( Q_\theta(s_t,\mathbf{a}_{t:t+h-1}) -   \sum_{k=0}^{h-1}\gamma^{k} r_{t+k} -\gamma^hQ_{\bar{\theta}}(s_{t+h},\mathbf{a}_{t+h:t+2h-1}) \right)^2 \right]
    $$

    $$
    \mathcal{L}(\psi)= - \operatorname*{\mathbb{E}}_{s_t\sim\mathcal{D},  \mathbf{a}_{t:t+h-1}^\pi\sim\pi(\cdot \mid s_t)} \left[ Q_\theta(s_t, \mathbf{a}_{t:t+h-1}^\pi) \right]
    $$

<br>

- Offline2onlineì„ í•  ë•Œ ì¼ë°˜ì ì¸ online RLì•Œê³ ë¦¬ì¦˜ì„ ì„ íƒí•˜ê³  ìœ„ì²˜ëŸ¼ í•™ìŠµí•˜ë©´ ì„±ëŠ¥ì´ ì˜¤íˆë ¤ ë–¨ì–´ì§„ë‹¤.  (ë¬¸ì œì˜ ì›ì¸ ì„¤ëª… ë¶€ì¡±)
    - RLPD (Reinforcement Learning with Prior Data): replay bufferì— offline dataset ë„£ê³  online RLí•˜ëŠ” offline2online ê¸°ë²•.
    
    ![image.png]({{site.baseurl}}/assets/img/250724/image%201.png)
    

<br>

- $\pi_\psi(\mathbf{a}_{t:t+h-1} \mid s_t)$ì´ ì¶©ë¶„íˆ ë³µì¡í•œ ë¶„í¬ë©´ ì¢‹ê² ìŒ **â‡’ FQL ì†ì‹¤í•¨ìˆ˜ ì‚¬ìš©**
    - FQL ì†ì‹¤í•¨ìˆ˜: offline datasetìœ¼ë¡œë¶€í„° behavior cloningí•œ pretrained flow matching networkì™€ $\pi_\psi(\mathbf{a}_{t:t+h-1} \mid s_t)$ì˜ behavior constrained term ì¶”ê°€ëœ í˜•íƒœ
        
        ![image.png]({{site.baseurl}}/assets/img/250724/image%202.png)
        

<br>

---

# Experiment

**Benchmark environment:** 

- OGBenchì—ì„œ 5ê°œ domain $\times$ ê° domainì—ì„œ 2ê°œ tasks
- Robomimicì—ì„œ 2ê°œ tasks

![image.png]({{site.baseurl}}/assets/img/250724/image%203.png)

<br>

**Learning curves**

![image.png]({{site.baseurl}}/assets/img/250724/image%204.png)

<br>

**Ablations studies**

![image.png]({{site.baseurl}}/assets/img/250724/image%205.png)

- ì™¼ìª½: Chunk sizeì— ë”°ë¥¸ ablation
- ì˜¤ë¥¸ìª½: Action chunkingì´ explorationì„ ì˜í–ˆë‹¤ëŠ” ì¦ê±°
    - `cube-triple` ë„ë©”ì¸ì—ì„œ FQL ì•Œê³ ë¦¬ì¦˜ê³¼ Q-LAC ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ê°ê° 1M steps ë™ì•ˆ offline pretrainingí•œ í›„ ë°ì´í„° 1M ê°œ ìˆ˜ì§‘. $\mathcal{D}_1, \mathcal{D}_5$
    - $\mathcal{D}_1^{\text{comp}}=$ Offline dataset + $\mathcal{D}_1$, $\mathcal{D}_5^{\text{comp}}=$ Offline dataset + $\mathcal{D}_5$ ìœ¼ë¡œ ë‹¤ì‹œ offline ptrainingí•œ ê²°ê³¼

<br>

# Reference

<a name="ref1"></a> [1] Park, Seohong, Qiyang Li, and Sergey Levine. "Flow q-learning."Â *arXiv preprint arXiv:2502.02538*Â (2025).

<a name="ref2"></a> [2] Zhao, Tony Z., et al. "Learning fine-grained bimanual manipulation with low-cost hardware."Â *arXiv preprint arXiv:2304.13705*Â (2023). [https://tonyzhaozh.github.io/aloha/](https://tonyzhaozh.github.io/aloha/)

<br>

---